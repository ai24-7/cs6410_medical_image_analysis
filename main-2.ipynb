{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b94b11-8387-42a6-bd00-f6c85fb89bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 08:02:08.015359: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(422, 224, 224, 3)\n",
      "(422, 224, 224, 22)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "\n",
    "def load_data_from_files(output_dir, image_shape=(224, 224, 3), mask_shape=(224, 224, 22)):\n",
    "    images = []\n",
    "    masks = []\n",
    "    for filename in sorted(os.listdir(output_dir)):\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        if filename.endswith('_image.txt'):\n",
    "            # Load flattened image and reshape\n",
    "            flattened = np.loadtxt(filepath)\n",
    "            image = flattened.reshape(image_shape)\n",
    "            images.append(image)\n",
    "        elif filename.endswith('_mask.txt'):\n",
    "            # Load flattened mask and reshape\n",
    "            flattened = np.loadtxt(filepath)\n",
    "            mask = flattened.reshape(mask_shape)\n",
    "            masks.append(mask)\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "\n",
    "# Load processed data\n",
    "output_dir = 'output_o'\n",
    "images, masks = load_data_from_files(output_dir)\n",
    "print(images.shape)\n",
    "print(masks.shape)\n",
    "\n",
    "# Split the data\n",
    "train_imgs, test_imgs, train_masks, test_masks = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "train_imgs, val_imgs, train_masks, val_masks = train_test_split(train_imgs, train_masks, test_size=0.125, random_state=42)  # 0.125 x 0.8 = 0.1\n",
    "\n",
    "# Initialize and train the U-Net model\n",
    "# model = unet_model()\n",
    "# history = model.fit(train_imgs, train_masks, validation_data=(val_imgs, val_masks), batch_size=16, epochs=25)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# model.evaluate(test_imgs, test_masks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1a5d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(422, 224, 224, 3)\n",
      "(422, 224, 224, 22)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_data(directory):\n",
    "    images = []\n",
    "    masks = []\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        if filename.endswith('_image.npy'):\n",
    "            images.append(np.load(os.path.join(directory, filename)))\n",
    "        elif filename.endswith('_mask.npy'):\n",
    "            masks.append(np.load(os.path.join(directory, filename)))\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(images, masks, train_size=0.8, test_size=0.1, random_state=42):\n",
    "    # Split into train and remaining (test + validation)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        images, masks, train_size=train_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "    # Split the remaining into test and validation\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=random_state, shuffle=True)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "output_dir = 'output'\n",
    "images, masks = load_data(output_dir)\n",
    "print(images.shape)\n",
    "print(masks.shape)\n",
    "\n",
    "# Split the data\n",
    "train_imgs, val_imgs, test_imgs, train_masks, val_masks, test_masks = split_data(images, masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f88c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_imgs.shape)\n",
    "print(train_masks.shape)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_sample(image, mask, title=\"\"):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if image.ndim == 3:  # Check if the mask is one-hot encoded\n",
    "        image = np.argmax(image, axis=-1)  # Convert one-hot encoded mask to single-channel\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.title('Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    if mask.ndim == 3:  # Check if the mask is one-hot encoded\n",
    "        mask = np.argmax(mask, axis=-1)  # Convert one-hot encoded mask to single-channel\n",
    "    plt.imshow(mask, cmap='jet')  # Using 'jet' to provide distinct colors to different classes\n",
    "    plt.title('Mask')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'images' and 'masks' are your datasets\n",
    "for i in range(15):  # Display first 5 pairs to check\n",
    "    plot_sample(train_imgs[i], train_masks[i], title=f\"Sample {i+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Define Dice loss function and coefficient\n",
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "def unet_model(input_size=(224, 224, 3), num_classes=22):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Downward path\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Dropout(0.1)(c1)\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Dropout(0.1)(c2)\n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "    # Upward path\n",
    "    u4 = UpSampling2D((2, 2))(c3)\n",
    "    u4 = concatenate([u4, c2])\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(u4)\n",
    "    c4 = Dropout(0.1)(c4)\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(c4)\n",
    "    \n",
    "    u5 = UpSampling2D((2, 2))(c4)\n",
    "    u5 = concatenate([u5, c1])\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(u5)\n",
    "    c5 = Dropout(0.1)(c5)\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(c5)\n",
    "    \n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='sigmoid')(c5)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-5), loss=dice_loss, metrics=[dice_coefficient])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize the U-Net model\n",
    "model = unet_model()\n",
    "model.summary()  # This will print the summary of the model without needing Graphviz\n",
    "\n",
    "# Assume data preparation here (X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(train_imgs, train_masks, validation_data=(val_imgs, val_masks), batch_size=8, epochs=50)\n",
    "\n",
    "# Plot training and validation Dice loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Dice Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd16492f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def conv_block(input_tensor, num_filters):\n",
    "    \"\"\"A block of two convolutional layers with ReLU activations and batch normalization.\"\"\"\n",
    "    x = Conv2D(num_filters, (3, 3), activation='relu', padding='same')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(num_filters, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "    \"\"\"An encoder block with a convolution block followed by max pooling.\"\"\"\n",
    "    x = conv_block(input_tensor, num_filters)\n",
    "    p = MaxPooling2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "    \"\"\"A decoder block with upsampling, concatenation, and a convolution block.\"\"\"\n",
    "    x = UpSampling2D((2, 2))(input_tensor)\n",
    "    x = concatenate([x, concat_tensor])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def unet_model(input_size=(224, 224, 3), num_classes=22):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Encoder\n",
    "    c1, p1 = encoder_block(inputs, 64)\n",
    "    c2, p2 = encoder_block(p1, 128)\n",
    "    c3, p3 = encoder_block(p2, 256)\n",
    "    c4, p4 = encoder_block(p3, 512)\n",
    "    \n",
    "    # Bridge\n",
    "    b = conv_block(p4, 1024)\n",
    "\n",
    "    # Decoder\n",
    "    d1 = decoder_block(b, c4, 512)\n",
    "    d2 = decoder_block(d1, c3, 256)\n",
    "    d3 = decoder_block(d2, c2, 128)\n",
    "    d4 = decoder_block(d3, c1, 64)\n",
    "\n",
    "    # Output\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(d4)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the U-Net model\n",
    "model = unet_model()\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(train_imgs, train_masks, validation_data=(val_imgs, val_masks), batch_size=8, epochs=50)\n",
    "\n",
    "# Plot training and validation Dice loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.title('Training and Validation Dice Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
